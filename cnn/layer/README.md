# layer 模块文档
## 1. 全连接层（Linear）
### 1.1 基本概念
线性层实现经典的仿射变换：  
$$
\text{output} = XW + b
$$  
其中：
- $ X $ 是输入张量，形状为 `(batch_size, in_features)`
- $ W $ 是权重参数，形状为 `(in_features, out_features)`
- $ b $ 是偏置参数，初始形状为 `(1, out_features)`，将自动通过广播扩展到 batch 尺寸

### 1.2 参数说明：
| 参数名        | 类型     | 说明                                 |
|---------------|----------|--------------------------------------|
| `in_features` | `int`    | 输入特征维度                         |
| `out_features`| `int`    | 输出特征维度                         |

### 1.3 设计说明：

- **参数封装**：权重和偏置均封装为 `Parameter` 对象，支持注册和自动求导。
- **初始化策略**：默认使用 He 初始化（适合 ReLU 激活）。
### 1.4 示例代码：

```python
layer = Linear(64, 10)
out = layer(x)  # x: Tensor of shape (batch_size, 64)
```

---
当然，来一波硬核又清晰的卷积操作原理讲解 💡  
你很快就会发现，这其实是数学优雅和工程高效的结合。

---

## 2. 卷积层（Conv2D）

### 2.1 基本概念
**卷积操作**本质上是滑动一个小的“过滤器”（kernel）在输入特征图上提取局部特征。

类似于一个“窗口”在图像上滑动，每次对窗口区域内的像素和 kernel 做一个**逐元素乘法 + 累加**，最后输出一个值。

---

### 2.2 数学公式

设：

- 输入特征图 $ X \in \mathbb{R}^{C_{in} \times H \times W} $
- 卷积核 $ K \in \mathbb{R}^{C_{out} \times C_{in} \times k_H \times k_W} $
- 偏置 $ b \in \mathbb{R}^{C_{out}} $

输出的第 $n$ 个样本、第 $c$ 个通道、位置 $(i,j)$ 为：

$$
Y_{n, c, i, j} = \sum_{d=1}^{C_{in}} \sum_{u=1}^{k_H} \sum_{v=1}^{k_W} X_{n, d, i+u, j+v} \cdot K_{c, d, u, v} + b_c
$$

这个操作在图像上滑动，把每一小块都映射成一个数。

---

### 2.3 参数解释

1. `in_channels` / `out_channels`

- `in_channels`: 输入图像的通道数，灰度图为 1，RGB 图为 3。
- `out_channels`: 卷积核个数。每个卷积核产生一张**输出特征图**。

👉 控制输出张量的深度！

2. `kernel_size` (或称 filter size)

- 决定滑动窗口的大小，常见有 (3×3), (5×5)。
- 越大捕捉范围越广，但计算开销增加。

3. `stride`

- 滑动步长，控制 kernel 每次移动几个像素。
- 默认为 1。如果设为 2，则输出会缩小一半。

4. `padding`

- 为了避免图像越卷越小，在边缘补 0。
- `valid`：不补，越卷越小。
- `same`：补到输出尺寸相同。

5. `bias`

- 每个输出通道加一个标量偏置，提升模型表达能力。

---